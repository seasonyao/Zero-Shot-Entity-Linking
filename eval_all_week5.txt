1.256 batch_size and 768 token pretrain会oom. so 用128的batch size

2.一种新的训练方式：输入64个candidates，但是进入bert的只有16个，得到bert 16个的输出以后随机扩充成1+63=64的vector，就可以训练+测试64了。每次64选16都是随机的，保证model可以看到足够的信息。依旧需要增加gradient accumulate来对应batch size的影响。另外设计一种训练模式来预训练position embeddding layer