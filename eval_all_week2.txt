关于tpu的尝试处理
# run_config = tf.estimator.tpu.RunConfig(
#     cluster=tpu_cluster_resolver,
#     master=FLAGS.master,
#     model_dir=FLAGS.output_dir,
#     save_checkpoints_steps=FLAGS.save_checkpoints_steps,
#     tpu_config=tf.estimator.tpu.TPUConfig(
#         iterations_per_loop=FLAGS.iterations_per_loop,
#         per_host_input_for_training=is_per_host,
#         num_cores_per_replica=8,
#         input_partition_dims = [{'input_ids': [1,8,1], 'input_mask': [1,8,1], 
结果：
21.46G (1,8,1) in v3-8 bs=1
135.99G (1,8,1) in v3-8 bs=8
21.42G (1,1,1) in v3-8 bs=8
21.42G nothing in v3-8 bs=8
44.14G nothing in v3-8 bs=16

V3-8最大16g


------------------------------------------------------------------------------------------
m40 single tpu:
		bert(max_sequence=256, batch_size only 2~3, can16)
		albert_base(max_sequence=256, batch_size about 7~8, can16)

Bert tpu ms512 cn64:
Total hbm usage >= 21.42G:
    reserved        528.00M
    program          20.91G
    arguments       unknown size

Output size unknown.

Program hbm requirement 20.91G:
    reserved           4.0K
    global           420.0K
    HLO temp         20.91G (100.0% utilization, 0.5% fragmentation (104.47M))

  Largest program allocations in hbm:

  1. Size: 768.00M
     Shape: f32[64,12,512,512]{3,2,1,0:T(8,128)}
     Unpadded size: 768.00M
     XLA label: %fusion.6236 = (f32[64,12,512]{2,1,0:T(8,128)}, f32[64,12,512,512]{3,2,1,0:T(8,128)}) fusion(f32[64,12,512]{2,1,0:T(8,128)} %fusion.1416, f32[64,12,512]{2,1,0:T(8,128)} %get-tuple-element.17943, f32[64,512]{1,0:T(8,128)} %fusion.4655, f32[64,12,512,512]{3,...
     Allocation type: HLO temp
     ==========================

  2. Size: 768.00M
     Operator: op_type="RandomUniform" op_name="bert/encoder/layer_0/attention/self/dropout/random_uniform/RandomUniform"
     Shape: f32[64,12,512,512]{3,2,1,0:T(8,128)}
     Unpadded size: 768.00M
     XLA label: %rng.117 = f32[64,12,512,512]{3,2,1,0:T(8,128)} rng(f32[]{:T(256)} %constant.7637, f32[]{:T(256)} %constant.7634), distribution=rng_uniform, metadata={op_type="RandomUniform" op_name="bert/encoder/layer_0/attention/self/dropout/random_uniform/RandomUniform...
     Allocation type: HLO temp
     ==========================

  3. Size: 768.00M
     Operator: op_type="RandomUniform" op_name="bert/encoder/layer_1/attention/self/dropout/random_uniform/RandomUniform"
     Shape: f32[64,12,512,512]{3,2,1,0:T(8,128)}
     Unpadded size: 768.00M
     XLA label: %rng.120 = f32[64,12,512,512]{3,2,1,0:T(8,128)} rng(f32[]{:T(256)} %constant.7637, f32[]{:T(256)} %constant.7634), distribution=rng_uniform, metadata={op_type="RandomUniform" op_name="bert/encoder/layer_1/attention/self/dropout/random_uniform/RandomUniform...
     Allocation type: HLO temp

总结来说，出现oom最关键得地方在于tf.gradient函数在计算整个graph的gradient的时候需要的显存太大了。所以如果用model parallel在前面create model部分做各种分散都没效果。我试着拿单独整个m40或者cpu去单独承担tf.gradient也无法容纳candidate_num64+max_sequence512的需求。所以首先我找到了一个来自openai的叫做checkpoint-gradient的东西，但对应的问题是这个gradient方式跑的实在太慢了。还有一些其他的方式。

疑问：1.window size的不同是否只对最前面的embedding和transformer的第一层有影响？所以重点处理embedding到transformer的第一层之间这段operation就可以减少gradient所需的计算？

2.embedding_postprocessor
output = input + token_type_embeddings + mention_id + position_embeddings
是否是直接将原来token的embedding加上这些东西，所以这层输出会变大？

------------------------------------------------------------------------------------------

Bert ms256_64:  7584						0.74248495511
	coronation_street 1048
		eval_accuracy = 0.8253817
		eval_loss = 1.374492
		global_step = 18750
		loss = 1.0880518

	ice_hockey 1476
		eval_accuracy = 0.72961956
		eval_loss = 2.8742874
		global_step = 18750
		loss = 2.4326644

	muppets 1627
		eval_accuracy = 0.79679805
		eval_loss = 1.6510948
		global_step = 18750
		loss = 1.6400665

	elder_scrolls 3433
		eval_accuracy = 0.6969697
		eval_loss = 2.4781182
		global_step = 18750
		loss = 2.4632294

Bert ms512_48:
很有可能发生的原因：384 128有有很多padding，255 255就会好很多。而在albert384 128也好的原因是sentence piece对比wordpiece相同情况下少很多padding。

	train384_128 test384_128:  				0.73389249086
		coronation_street:
			eval_accuracy = 0.7912088
			eval_loss = 1.7343221
			global_step = 18750
			loss = 1.7325134
		ice_hockey:
			eval_accuracy = 0.71146953
			eval_loss = 3.0319192
			global_step = 18750
			loss = 2.8942568
		muppets:
			eval_accuracy = 0.77272725
			eval_loss = 1.8869402
			global_step = 18750
			loss = 1.7082815
		elder_scrolls:
			eval_accuracy = 0.7076311
			eval_loss = 2.5060909
			global_step = 18750
			loss = 2.7518344

	train384_128 test255_255:				0.73946967905
		coronation_street:
			eval_accuracy = 0.82623625
			eval_loss = 1.5390313
			global_step = 18750
			loss = 1.530455		
		ice_hockey:
			eval_accuracy = 0.70654124
			eval_loss = 3.1011977
			global_step = 18750
			loss = 3.3235602
		muppets:
			eval_accuracy = 0.78310275
			eval_loss = 1.823563
			global_step = 18750
			loss = 1.4829438
		elder_scrolls:
			eval_accuracy = 0.70646065
			eval_loss = 2.4674952
			global_step = 18750
			loss = 2.5930345

	train384_128 test128_384:				0.73196848287
		coronation_street:
			eval_accuracy = 0.8385989
			eval_loss = 1.2141138
			global_step = 18750
			loss = 1.3624961	
		ice_hockey:
			eval_accuracy = 0.6886201
			eval_loss = 3.4567566
			global_step = 18750
			loss = 3.353269
		muppets:
			eval_accuracy = 0.78507906
			eval_loss = 1.771818
			global_step = 18750
			loss = 1.3788221
		elder_scrolls:
			eval_accuracy = 0.6928839
			eval_loss = 2.6585417
			global_step = 18750
			loss = 2.7140794


	train255_255 test255_255:				0.74489890202
		coronation_street:
			eval_accuracy = 0.80906594
			eval_loss = 1.4221852
			global_step = 18750
			loss = 1.1246779	
		ice_hockey:
			eval_accuracy = 0.71505374
			eval_loss = 2.7103276
			global_step = 18750
			loss = 2.882507
		muppets:
			eval_accuracy = 0.7751976
			eval_loss = 1.9692776
			global_step = 18750
			loss = 1.5692787
		elder_scrolls:
			eval_accuracy = 0.7237828
			eval_loss = 2.4390965
			global_step = 18750
			loss = 2.6193678

	train255_255 test384_128:				0.71717016297
		coronation_street:
			eval_accuracy = 0.7630494
			eval_loss = 1.9843526
			global_step = 18750
			loss = 1.7517833
		ice_hockey:
			eval_accuracy = 0.6863799
			eval_loss = 2.9277663
			global_step = 18750
			loss = 3.0596657
		muppets:
			eval_accuracy = 0.7559289
			eval_loss = 2.140228
			global_step = 18750
			loss = 1.7565297
		elder_scrolls:
			eval_accuracy = 0.6980337
			eval_loss = 2.6708722
			global_step = 18750
			loss = 3.084367

	train255_255 test128_384:				0.72702510605
		coronation_street:
			eval_accuracy = 0.80494505
			eval_loss = 1.4851861
			global_step = 18750
			loss = 1.1629953
		ice_hockey:
			eval_accuracy = 0.6778674
			eval_loss = 3.0377393
			global_step = 18750
			loss = 3.0686662
		muppets:
			eval_accuracy = 0.7618577
			eval_loss = 1.9639674
			global_step = 18750
			loss = 1.4733629
		elder_scrolls:
			eval_accuracy = 0.7078652
			eval_loss = 2.5683317
			global_step = 18750
			loss = 2.810178

